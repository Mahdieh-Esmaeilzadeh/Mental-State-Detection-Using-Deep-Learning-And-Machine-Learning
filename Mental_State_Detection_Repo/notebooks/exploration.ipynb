{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mental State Detection - Exploratory Notebook\n",
    "\n",
    "EDA and feature extraction for **Mental State Detection from Speech**.\n",
    "- MFCC, spectral, pitch, ZCR, RMS features\n",
    "- Data augmentation hooks included\n",
    "- Ready for further training notebooks\n",
    "\n",
    "Notes:\n",
    "- Adjust `DATA_PATH` to point to your `.wav` dataset organized as `DATA_PATH/<label>/*.wav`.\n",
    "- This notebook focuses on feature extraction and exploratory analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Imports\n",
    "import os, random, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import librosa, librosa.display\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12,7)\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Parameters & Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"Bale_records\"   # path to dataset\n",
    "AUGMENT_FACTOR = 2  # how many augmented copies per file\n",
    "SR = 16000\n",
    "N_MFCC = 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Feature Extraction Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_file(file_path, target_sr=SR, n_mfcc=N_MFCC):\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=target_sr)\n",
    "        if y is None or y.size == 0:\n",
    "            return None\n",
    "        y = librosa.util.normalize(y)\n",
    "        feats = []\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "        feats.extend(np.mean(mfcc, axis=1).tolist())\n",
    "        feats.extend(np.std(mfcc, axis=1).tolist())\n",
    "        feats.append(np.mean(librosa.feature.spectral_centroid(y=y, sr=sr)))\n",
    "        feats.append(np.mean(librosa.feature.spectral_bandwidth(y=y, sr=sr)))\n",
    "        feats.append(np.mean(librosa.feature.spectral_rolloff(y=y, sr=sr)))\n",
    "        feats.append(np.mean(librosa.feature.zero_crossing_rate(y)))\n",
    "        feats.append(np.mean(librosa.feature.rms(y=y)))\n",
    "        pitches, mags = librosa.piptrack(y=y, sr=sr)\n",
    "        mag_mask = mags > np.median(mags[mags > 0]) if np.any(mags>0) else mags > 0\n",
    "        pitch_vals = pitches[mag_mask]\n",
    "        feats.append(np.median(pitch_vals) if pitch_vals.size > 0 else 0.0)\n",
    "        return np.array(feats, dtype=float)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def load_dataset_features(data_path=DATA_PATH):\n",
    "    X_list, y_list = [], []\n",
    "    for root, dirs, files in os.walk(data_path):\n",
    "        if root == data_path: continue\n",
    "        label = os.path.basename(root)\n",
    "        for f in files:\n",
    "            if not f.lower().endswith('.wav'): continue\n",
    "            fp = os.path.join(root, f)\n",
    "            feats = extract_features_file(fp)\n",
    "            if feats is None: continue\n",
    "            X_list.append(feats)\n",
    "            y_list.append(label)\n",
    "    X = np.vstack(X_list)\n",
    "    y = np.array(y_list)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Load Dataset & Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y_labels = load_dataset_features(DATA_PATH)\n",
    "print('Samples:', X.shape[0], 'Features:', X.shape[1])\n",
    "print('Classes:', np.unique(y_labels))\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y_labels)\n",
    "classes = le.classes_\n",
    "print('Encoded classes:', classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution\n",
    "plt.figure(figsize=(12,5))\n",
    "sns.countplot(x=y_labels, order=classes)\n",
    "plt.title('Class distribution')\n",
    "plt.show()\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "plt.figure(figsize=(10,7))\n",
    "for i, cls in enumerate(classes):\n",
    "    plt.scatter(X_pca[y_enc==i,0], X_pca[y_enc==i,1], label=cls, alpha=0.7)\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.title('PCA of Audio Features')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
